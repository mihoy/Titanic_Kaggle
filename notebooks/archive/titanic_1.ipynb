{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# machine learning tools\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,recall_score,precision_score,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('input/train.csv')\n",
    "test_df = pd.read_csv('input/test.csv')\n",
    "combine = [train_df, test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PassengerId' 'Survived' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch'\n",
      " 'Ticket' 'Fare' 'Cabin' 'Embarked']\n",
      "['PassengerId' 'Pclass' 'Name' 'Sex' 'Age' 'SibSp' 'Parch' 'Ticket' 'Fare'\n",
      " 'Cabin' 'Embarked']\n"
     ]
    }
   ],
   "source": [
    "print(train_df.columns.values)\n",
    "print(test_df.columns.values)\n",
    "\n",
    "# print(train_df.head(5))\n",
    "# print(test_df.head(5))\n",
    "\n",
    "# print(train_df.info())\n",
    "# print(test_df.info())\n",
    "# for df in combine:\n",
    "#     print(df.describe(include=[np.object]))\n",
    "#print(test_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_id = test_df['PassengerId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(train_df[['Cabin', 'Survived']].groupby(['Cabin'], as_index=False).mean().sort_values(by='Survived', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['left'] = dataset.Cabin.str[0]\n",
    "    dataset['left'] = np.where(dataset['left'].isin(['A','B','C','D','E','F']), dataset['left'], 0)\n",
    "    # dataset['left'] = dataset['left'].replace('A', 1).replace('B', 2).replace('C', 3).replace('D', 4).replace('E', 5).replace('F', 6)\n",
    "    dataset['left'].replace({'A':1, 'B':2, 'C':3, 'D':4, 'E':5, 'F':6}, inplace = True)\n",
    "    # dataset['left'].replace('A', 1, inplace=True).replace('B', 2, inplace=True).replace('C', 3, inplace=True).replace('D', 4, inplace=True).replace('E', 5, inplace=True).replace('F', 6, inplace=True)\n",
    "    #print(dataset['left'])\n",
    "    \n",
    "# print(\"Before\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n",
    "\n",
    "#print(train_df['Ticket'].value_counts())\n",
    "\n",
    "for dataset in combine:\n",
    "    dataset['ticket_left'] = dataset['Ticket'].str[0]\n",
    "    dataset['ticket_left'] = np.where(dataset['ticket_left'].isin(['1','2','3','A','C','P','S']),dataset['ticket_left'], 0)\n",
    "    dataset['ticket_left'].replace({'1':1,'2':2,'3':3,'0':0,'P':0, 'C':3, 'S':3,  'A':3}, inplace = True)\n",
    "\n",
    "\n",
    "#a = train_df[['ticket_left', 'Survived']].groupby(['ticket_left'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n",
    "# b = train_df[['ticket_left', 'Survived']].groupby(['ticket_left'], as_index=False).count().sort_values(by='Survived', ascending=False)\n",
    "\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "test_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "\n",
    "# print(\"After\", train_df.shape, test_df.shape, combine[0].shape, combine[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch     Fare Embarked  left  ticket_left  Title  \n",
      "0      0   7.2500        S     0            3      1  \n",
      "1      0  71.2833        C     3            0      3  \n",
      "2      0   7.9250        S     0            3      2  \n",
      "3      0  53.1000        S     3            1      3  \n",
      "4      0   8.0500        S     0            3      1  \n"
     ]
    }
   ],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n",
    " \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "    \n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n",
    "for dataset in combine:\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "print(train_df.head())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Title  Survived\n",
      "0      1  0.156673\n",
      "1      2  0.702703\n",
      "2      3  0.793651\n",
      "3      4  0.575000\n",
      "4      5  0.347826\n"
     ]
    }
   ],
   "source": [
    "print(train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((891, 11), (418, 11))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\n",
    "test_df = test_df.drop(['Name'], axis=1)\n",
    "combine = [train_df, test_df]\n",
    "train_df.shape, test_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset[\"Age\"].fillna(train_df.Age.median(), inplace=True) \n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "    dataset['Embarked'] = dataset['Embarked'].map({'C':1, 'S':2, 'Q':3}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for dataset in combine:\n",
    "    dataset[\"FamilySize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\n",
    "    dataset[\"IsAlone\"]=0\n",
    "    dataset[\"IssmallFamily\"]=0\n",
    "    dataset[\"IslargeFamily\"]=0 \n",
    "    dataset.loc[dataset[\"FamilySize\"]==1,\"IsAlone\"]=1\n",
    "    dataset.loc[(dataset['FamilySize'] <= 4) & (dataset['FamilySize'] > 1),\"IssmallFamily\"]=1\n",
    "    dataset.loc[dataset[\"FamilySize\"]>4,\"IslargeFamily\"]=1\n",
    "    dataset.drop([\"FamilySize\",\"SibSp\",\"Parch\"], axis=1)  \n",
    "    # dataset.loc[dataset['FamilySize'] <= 1, 'FamSize'] = \"1\"\n",
    "    # dataset.loc[(dataset['FamilySize'] <= 4) & (dataset['FamilySize'] > 1), 'FamSize'] = \"2\"\n",
    "    # dataset.loc[(dataset['FamilySize'] > 4), 'FamSize'] = \"3\"\n",
    "\n",
    "#print(train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())\n",
    "# print(train_df[['FamSize', 'Survived']].groupby(['FamSize'], as_index=False).mean())\n",
    "#train_df = pd.get_dummies(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      "Survived         891 non-null int64\n",
      "Pclass           891 non-null int64\n",
      "Sex              891 non-null int64\n",
      "Age              891 non-null float64\n",
      "SibSp            891 non-null int64\n",
      "Parch            891 non-null int64\n",
      "Fare             891 non-null float64\n",
      "Embarked         891 non-null int64\n",
      "left             891 non-null int64\n",
      "ticket_left      891 non-null int64\n",
      "Title            891 non-null int64\n",
      "FamilySize       891 non-null int64\n",
      "IsAlone          891 non-null int64\n",
      "IssmallFamily    891 non-null int64\n",
      "IslargeFamily    891 non-null int64\n",
      "dtypes: float64(2), int64(13)\n",
      "memory usage: 104.5 KB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 418 entries, 0 to 417\n",
      "Data columns (total 15 columns):\n",
      "PassengerId      418 non-null int64\n",
      "Pclass           418 non-null int64\n",
      "Sex              418 non-null int64\n",
      "Age              418 non-null float64\n",
      "SibSp            418 non-null int64\n",
      "Parch            418 non-null int64\n",
      "Fare             418 non-null float64\n",
      "Embarked         418 non-null int64\n",
      "left             418 non-null int64\n",
      "ticket_left      418 non-null int64\n",
      "Title            418 non-null int64\n",
      "FamilySize       418 non-null int64\n",
      "IsAlone          418 non-null int64\n",
      "IssmallFamily    418 non-null int64\n",
      "IslargeFamily    418 non-null int64\n",
      "dtypes: float64(2), int64(13)\n",
      "memory usage: 49.1 KB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.975308641975\n"
     ]
    }
   ],
   "source": [
    "#提出用\n",
    "\n",
    "# random_state=5\n",
    "# X_train, X_val, Y_train, Y_val = train_test_split(combine[0].drop([\"Survived\"], axis=1)\\\n",
    "#                 , combine[0][\"Survived\"], train_size=0.8, random_state=random_state)\n",
    "\n",
    "test_df['Fare'].fillna(0,inplace=True)\n",
    "test = test_df.drop(['PassengerId'],axis=1)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_df.drop(['Survived'],axis=1), train_df['Survived'])\n",
    "Y_val_pred = clf.predict(test)\n",
    "print(clf.score(train_df.drop(['Survived'],axis=1), train_df['Survived']))\n",
    "\n",
    "\n",
    "predictions = pd.DataFrame({\n",
    "        \"PassengerId\": p_id,\n",
    "        \"Survived\": Y_val_pred,\n",
    "    })\n",
    "\n",
    "predictions.to_csv(\"output/submit2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# データ検証用\n",
    "#\n",
    "#x_train = トレインデータの学習データ\n",
    "#x_val = テストデータの　学習データ\n",
    "#Y_train = とれいんデータの答え\n",
    "#Y_val = テストデータの答え\n",
    "\n",
    "random_state=5\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(combine[0].drop([\"Survived\"], axis=1)\\\n",
    "                , combine[0][\"Survived\"], train_size=0.8, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "# random_forest = RandomForestClassifier(n_estimators=100,random_state = random_state,criterion='entropy',max_depth=5, min_samples_leaf=3)\n",
    "# random_forest.fit(X_train, Y_train)\n",
    "# Y_train_pred = random_forest.predict(X_train)\n",
    "# Y_val_pred = random_forest.predict(X_val)\n",
    "# acc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\n",
    "# cm = confusion_matrix(Y_val, Y_val_pred)\n",
    "# print(cm)\n",
    "# print('Accuracy on Training Set: {:.3f}'.format(accuracy_score(Y_train, Y_train_pred)))\n",
    "# print('Accuracy on Validation Set: {:.3f}'.format(accuracy_score(Y_val, Y_val_pred)))\n",
    "# print('recall on Validation Set: {:.3f}'.format(recall_score(Y_val, Y_val_pred)))\n",
    "# print('precision on Validation Set: {:.3f}'.format(precision_score(Y_val, Y_val_pred)))\n",
    "# print('f1 on Validation Set: {:.3f}'.format(f1_score(Y_val, Y_val_pred)))\n",
    "# print(\"ランダムフォレスト={}\".format(acc_random_forest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 確率的勾配降下法\n",
    "# sgd = SGDClassifier(random_state = random_state)\n",
    "# sgd.fit(X_train, Y_train)\n",
    "# Y_train_pred = sgd.predict(X_train)\n",
    "# Y_val_pred = sgd.predict(X_val)\n",
    "# acc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\n",
    "# print('Accuracy on Training Set: {:.3f}'.format(accuracy_score(Y_train, Y_train_pred)))\n",
    "# print('Accuracy on Validation Set: {:.3f}'.format(accuracy_score(Y_val, Y_val_pred)))\n",
    "# print(\"SGD={}\".format(acc_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ロジスティック回帰\n",
    "# logreg = LogisticRegression(random_state = random_state)\n",
    "# logreg.fit(X_train, Y_train)\n",
    "# Y_train_pred = logreg.predict(X_train)\n",
    "# Y_val_pred = logreg.predict(X_val)\n",
    "# acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\n",
    "# cm = confusion_matrix(Y_val, Y_val_pred)\n",
    "# print(cm)\n",
    "# print('Accuracy on Training Set: {:.3f}'.format(accuracy_score(Y_train, Y_train_pred)))\n",
    "# print('Accuracy on Validation Set: {:.3f}'.format(accuracy_score(Y_val, Y_val_pred)))\n",
    "# print(\"ロジスティック回帰={}\".format(acc_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Machines\n",
    "# svc = SVC()\n",
    "# svc.fit(X_train, Y_train)\n",
    "# Y_train_pred = svc.predict(X_train)\n",
    "# Y_val_pred = svc.predict(X_val)\n",
    "# acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
    "# cm = confusion_matrix(Y_val, Y_val_pred)\n",
    "# print(cm)\n",
    "# print('Accuracy on Training Set: {:.3f}'.format(accuracy_score(Y_train, Y_train_pred)))\n",
    "# print('Accuracy on Validation Set: {:.3f}'.format(accuracy_score(Y_val, Y_val_pred)))\n",
    "# print(\"SVC={}\".format(acc_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mean: 0.68891, std: 0.05375, params: {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}, mean: 0.62615, std: 0.04625, params: {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}, mean: 0.79326, std: 0.04434, params: {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}, mean: 0.68350, std: 0.04606, params: {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}, mean: 0.78849, std: 0.03934, params: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}, mean: 0.78254, std: 0.03601, params: {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}, mean: 0.77562, std: 0.03793, params: {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}, mean: 0.80501, std: 0.04818, params: {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}]\n",
      "{'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.0001, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n",
      "[[100  11]\n",
      " [ 15  53]]\n",
      "Accuracy on Training Set: 0.822\n",
      "Accuracy on Validation Set: 0.855\n",
      "recall on Validation Set: 0.779\n",
      "precision on Validation Set: 0.828\n",
      "f1 on Validation Set: 0.803\n",
      "SVC=82.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mihoyamamoto/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py:667: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "params = [\n",
    "    {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.001, 0.0001]},\n",
    "    ]\n",
    "\n",
    "score = 'f1'\n",
    "clf = GridSearchCV(\n",
    "    SVC(), # 識別器\n",
    "    params, # 最適化したいパラメータセット \n",
    "    cv=5, # 交差検定の回数\n",
    "    scoring='%s_weighted' % score ) # モデルの評価関数の指定\n",
    "    \n",
    "clf.fit(X_train, Y_train)\n",
    "\n",
    "print(clf.grid_scores_)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_estimator_)\n",
    "# # # Support Vector Machines\n",
    "# svc = SVC(kernel='rbf',C=1000, gamma=0.00001)\n",
    "svc = clf.best_estimator_\n",
    "# svc = SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0,\n",
    "#   decision_function_shape='ovr', degree=3, gamma=0.0001, kernel='rbf',\n",
    "#   max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "#   tol=0.001, verbose=False)\n",
    "svc.fit(X_train, Y_train)\n",
    "Y_train_pred = svc.predict(X_train)\n",
    "Y_val_pred = svc.predict(X_val)\n",
    "acc_svc = round(svc.score(X_train, Y_train) * 100, 2)\n",
    "cm = confusion_matrix(Y_val, Y_val_pred)\n",
    "print(cm)\n",
    "print('Accuracy on Training Set: {:.3f}'.format(accuracy_score(Y_train, Y_train_pred)))\n",
    "print('Accuracy on Validation Set: {:.3f}'.format(accuracy_score(Y_val, Y_val_pred)))\n",
    "print('recall on Validation Set: {:.3f}'.format(recall_score(Y_val, Y_val_pred)))\n",
    "print('precision on Validation Set: {:.3f}'.format(precision_score(Y_val, Y_val_pred)))\n",
    "print('f1 on Validation Set: {:.3f}'.format(f1_score(Y_val, Y_val_pred)))\n",
    "print(\"SVC={}\".format(acc_svc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array length 179 does not match index length 418",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-ee828cd1b305>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m predictions = pd.DataFrame({\n\u001b[1;32m      2\u001b[0m         \u001b[0;34m\"PassengerId\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;34m\"Survived\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_val_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     })\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mihoyamamoto/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    264\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    265\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mihoyamamoto/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[0;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mihoyamamoto/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m   5396\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5397\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5398\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5399\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5400\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mihoyamamoto/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   5454\u001b[0m                     msg = ('array length %d does not match index length %d' %\n\u001b[1;32m   5455\u001b[0m                            (lengths[0], len(index)))\n\u001b[0;32m-> 5456\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5457\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5458\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array length 179 does not match index length 418"
     ]
    }
   ],
   "source": [
    "predictions = pd.DataFrame({\n",
    "        \"PassengerId\": p_id,\n",
    "        \"Survived\": Y_val_pred,\n",
    "    })\n",
    "\n",
    "predictions.to_csv(\"output/submit.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KNN(k近傍法)\n",
    "# knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "# knn.fit(X_train, Y_train)\n",
    "# Y_train_pred = knn.predict(X_train)\n",
    "# Y_val_pred = knn.predict(X_val)\n",
    "# acc_knn = round(knn.score(X_train, Y_train) * 100, 2)\n",
    "# cm = confusion_matrix(Y_val, Y_val_pred)\n",
    "# print(cm)\n",
    "# print('Accuracy on Training Set: {:.3f}'.format(accuracy_score(Y_train, Y_train_pred)))\n",
    "# print('Accuracy on Validation Set: {:.3f}'.format(accuracy_score(Y_val, Y_val_pred)))\n",
    "# print(\"knn={}\".format(acc_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Decision Tree\n",
    "# decision_tree = DecisionTreeClassifier(random_state = random_state,criterion='entropy', max_depth=10, min_samples_leaf=2)\n",
    "# decision_tree.fit(X_train, Y_train)\n",
    "# Y_pred_decision_tree = decision_tree.predict(X_test)\n",
    "# acc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\n",
    "# print(\"決定木={}\".format(acc_decision_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Perceptron\n",
    "# perceptron = Perceptron(random_state = random_state)\n",
    "# perceptron.fit(X_train, Y_train)\n",
    "# Y_pred_perceptron = perceptron.predict(X_test)\n",
    "# acc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\n",
    "# print(\"パーセプトロン={}\".format(acc_perceptron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # MLP(多層パーセプトロン)\n",
    "# mlp = MLPClassifier(solver=\"sgd\",random_state = random_state,max_iter=10000)\n",
    "# mlp.fit(X_train, Y_train)\n",
    "# # 結果表示\n",
    "# Y_train_pred = mlp.predict(X_train)\n",
    "# Y_val_pred = mlp.predict(X_val)\n",
    "# acc_mlp = round(mlp.score(X_train, Y_train) * 100, 2)\n",
    "# cm = confusion_matrix(Y_val, Y_val_pred)\n",
    "# print(cm)\n",
    "# print('Accuracy on Training Set: {:.3f}'.format(accuracy_score(Y_train, Y_train_pred)))\n",
    "# print('Accuracy on Validation Set: {:.3f}'.format(accuracy_score(Y_val, Y_val_pred)))\n",
    "# print(\"多層パーセプトロン={}\".format(acc_mlp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
